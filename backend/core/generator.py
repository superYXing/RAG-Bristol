import json
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from .config import settings
import logging
import time

logger = logging.getLogger("rag")

class RAGGenerator:
    def __init__(self):
        provider = (getattr(settings, "GENERATE_PROVIDER", "") or "qwen").lower()
        if provider == "qwen":
            model = settings.REWRITE_MODEL_NAME
            api_key = settings.REWRITE_API_KEY
            base_url = settings.REWRITE_BASE_URL
        elif provider == "gemini":
            model = getattr(settings, "GEMINI_MODEL_NAME", settings.GENERATE_MODEL_NAME)
            api_key = settings.GENERATE_API_KEY or settings.OPENAI_API_KEY or None
            base_url = settings.GENERATE_BASE_URL
        else:
            model = settings.GENERATE_MODEL_NAME
            api_key = settings.GENERATE_API_KEY or settings.OPENAI_API_KEY or None
            base_url = settings.GENERATE_BASE_URL

        self.llm = ChatOpenAI(
            model=model,
            api_key=api_key,
            base_url=base_url,
            streaming=True,
            temperature=0.3
        )
        
        self.prompt = ChatPromptTemplate.from_template("""
# Role
ä½ æ˜¯ä¸€ä¸ªå¸ƒé‡Œæ–¯æ‰˜å¤§å­¦ï¼ˆUniversity of Bristolï¼‰ä¸“ä¸šä¸”å‹å–„çš„æ ¡å›­å®¢æœåŠ©æ‰‹ã€‚ä½ çš„ç›®æ ‡æ˜¯ä¸ºå­¦ç”Ÿå’Œæ•™èŒå‘˜å·¥æä¾›åŸºäºå®˜æ–¹èµ„æ–™çš„å‡†ç¡®è§£ç­”ã€‚

markdownæ ¼å¼å‚è€ƒèµ„æ–™ï¼š
{context}

è¯·é’ˆå¯¹ç”¨æˆ·é—®é¢˜è¿›è¡Œæ·±åº¦æ•´åˆä¸æ€»ç»“ã€‚
ç”¨æˆ·é—®é¢˜ï¼š{query}

Constraints & Rules
1. çœŸå®æ€§åŸåˆ™ï¼šä»…æ ¹æ®æä¾›çš„å‚è€ƒèµ„æ–™å›ç­”ã€‚å¦‚æœèµ„æ–™ä¸­æœªæåŠç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®å‘ŠçŸ¥ç”¨æˆ·â€œæŠ±æ­‰ï¼Œç›®å‰çš„å‚è€ƒèµ„æ–™ä¸­æ²¡æœ‰å…³äºæ­¤é—®é¢˜çš„è¯¦ç»†ä¿¡æ¯â€ï¼Œä¸¥ç¦èƒ¡ä¹±ç¼–é€ ã€‚
2. å¤šè¯­è¨€é€‚é…ï¼šè‡ªåŠ¨è¯†åˆ«ç”¨æˆ·é—®é¢˜çš„è¯­è¨€ï¼ˆä¸­æ–‡ã€è‹±æ–‡æˆ–ä¸­è‹±æ··æ‚ï¼‰ï¼Œå¹¶å§‹ç»ˆä½¿ç”¨**å®Œå…¨ç›¸åŒçš„è¯­è¨€**è¿›è¡Œå›å¤ã€‚
3. Markdown æ ¼å¼è§„èŒƒï¼š
    - ä½¿ç”¨ Markdown è¯­æ³•ä¼˜åŒ–æ’ç‰ˆï¼šä½¿ç”¨åˆ†çº§æ ‡é¢˜ï¼ˆ###ï¼‰åˆ’åˆ†æ¨¡å—ã€åŠ ç²—ï¼ˆ**ï¼‰æ ¸å¿ƒæ¦‚å¿µã€ä½¿ç”¨åˆ—è¡¨ï¼ˆ- æˆ– 1.ï¼‰ç»„ç»‡ä¿¡æ¯ã€‚
    - æ‰€æœ‰é“¾æ¥å¿…é¡»è½¬æ¢ä¸º Markdown è¶…é“¾æ¥æ ¼å¼ã€‚
4. å¼•ç”¨ä¸æ¥æºæ ‡æ³¨ï¼š
    æ–‡ä¸­å¼•ç”¨ï¼šåœ¨æåŠç›¸å…³ä¿¡æ¯æ—¶ï¼Œå¿…é¡»åœ¨å¥æœ«ç´§è·Ÿå¼•ç”¨ç¼–å·ï¼Œæ ¼å¼ä¸º `[ç¼–å·]`ï¼ˆä¾‹å¦‚ï¼š[1]ï¼‰ã€‚
    æœ«å°¾åˆ—è¡¨ï¼šåœ¨å›ç­”ç»“æŸåçš„â€œå‚è€ƒé“¾æ¥â€éƒ¨åˆ†ï¼Œä½¿ç”¨ `[ç¼–å·] [æ ‡é¢˜](URL)` çš„ Markdown è¯­æ³•åˆ—å‡ºæ‰€æœ‰å¼•ç”¨æ¥æºï¼Œç¡®ä¿ç”¨æˆ·å¯ä»¥ç‚¹å‡»è·³è½¬ã€‚

# Response Format Example
### å…³äº [é—®é¢˜æ ¸å¿ƒè¯] çš„è§£ç­”
æ ¹æ®æœ€æ–°çš„æ ¡å›­é€šçŸ¥ **[1]**ï¼Œå¸ƒé‡Œæ–¯æ‰˜å¤§å­¦è®¡åˆ’åœ¨...
- **å…³é”®ç‚¹ A**ï¼šç›¸å…³å†…å®¹æè¿° [2]ã€‚
- **æ³¨æ„äº‹é¡¹**ï¼šè¯·åŠ¡å¿…äº[æ—¥æœŸ]å‰å®Œæˆç›¸å…³çš„ç”³è¯·æ‰‹ç»­ [1]ã€‚

---
### ğŸ”— å‚è€ƒé“¾æ¥
- [1] [å¸ƒé‡Œæ–¯æ‰˜å¤§å­¦å®˜æ–¹é€šçŸ¥ï¼šå…³äºXXXçš„è¯´æ˜](https://www.bristol.ac.uk/example-link-1)
- [2] [å­¦ç”Ÿæ”¯æŒä¸­å¿ƒæœåŠ¡æŒ‡å—](https://www.bristol.ac.uk/example-link-2)
""")

    async def generate_stream(self, query: str, docs: list, request_id: str = "", cache_update_callback=None):
        t0 = time.perf_counter()
        if not docs:
            yield "æœªæ‰¾åˆ°ç›¸å…³é€šçŸ¥ (No relevant notifications found)."
            logger.info(json.dumps({"event": "generate_empty", "request_id": request_id, "ms": round((time.perf_counter() - t0) * 1000, 2)}, ensure_ascii=False))
            return

        scores = [float((d.get("score") or 0.0)) for d in docs]
        reranks = [float((d.get("rerank_score") or 0.0)) for d in docs]
        best_score = max(scores) if scores else 0.0
        best_rerank = max(reranks) if reranks else 0.0
        if best_score < 0.5 or best_rerank <= 0.0:
            yield "æœªæ‰¾åˆ°ç›¸å…³é€šçŸ¥ (No relevant notifications found)."
            logger.info(json.dumps({
                "event": "generate_low_confidence",
                "request_id": request_id,
                "ms": round((time.perf_counter() - t0) * 1000, 2),
                "best_score": best_score,
                "best_rerank": best_rerank,
            }, ensure_ascii=False))
            return

        top_docs = []
        for i, doc in enumerate(docs[:3]):
            top_docs.append({
                "id": i + 1,
                "content": doc.get('content', ''),
                "metadata": doc.get('metadata', {}),
                "score": doc.get('rerank_score') or doc.get('score'),
                "date": doc.get('date')
            })
        
        source_msg = f"__SOURCES__:{json.dumps(top_docs)}\n"
        yield source_msg

        context_parts = []
        for i, doc in enumerate(docs[:3]):
            title = doc['metadata'].get('title', 'æ— æ ‡é¢˜')
            date = doc.get('date', 'æœªçŸ¥æ—¥æœŸ')
            url = doc['metadata'].get('url', 'æ— é“¾æ¥')
            content = doc['content']
            context_parts.append(f"[{i+1}] æ ‡é¢˜: {title} (æ—¥æœŸ: {date}, é“¾æ¥: {url})\nå†…å®¹: {content}\n")
        
        context_str = "\n".join(context_parts)
        logger.info(json.dumps({
            "event": "generate_start",
            "request_id": request_id,
            "docs_used": min(3, len(docs)),
            "context_chars": len(context_str),
            "prompt": "HIDDEN_IN_LOGS", # str(self.prompt)[:settings.LOG_PROMPT_MAX_CHARS],
        }, ensure_ascii=False))
        
        chain = self.prompt | self.llm
        
        cache_parts = [source_msg]
        answer_len = 0
        first_token_ms = None
        chunks_count = 0

        async for chunk in chain.astream({"query": query, "context": context_str}):
            piece = chunk.content or ""
            if first_token_ms is None:
                first_token_ms = round((time.perf_counter() - t0) * 1000, 2)
            yield piece
            cache_parts.append(piece)
            answer_len += len(piece)
            chunks_count += 1
        
        total_ms = round((time.perf_counter() - t0) * 1000, 2)
        logger.info(json.dumps({
            "event": "generate_end",
            "request_id": request_id,
            "ms": total_ms,
            "ttft": first_token_ms, # Time To First Token
            "chunks": chunks_count,
            "char_count": answer_len
        }, ensure_ascii=False))
            
        if cache_update_callback:
            try:
                if answer_len > 10:
                    cache_update_callback("".join(cache_parts))
            except Exception as e:
                logger.info(json.dumps({"event": "cache_update_error", "request_id": request_id, "error": str(e)}, ensure_ascii=False))

        logger.info(json.dumps({
            "event": "generate_end",
            "request_id": request_id,
            "ms": round((time.perf_counter() - t0) * 1000, 2),
            "first_token_ms": first_token_ms,
            "answer_chars": answer_len,
            "chunks": chunks_count,
        }, ensure_ascii=False))

rag_generator = RAGGenerator()
